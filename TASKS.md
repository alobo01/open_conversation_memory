# TASKS ‚Äî Roadmap v0‚Üív1

## üèóÔ∏è √âpica: Infraestructura y configuraci√≥n

### [INFRA-001] Configurar entorno de desarrollo
- **Resultado**: Entorno Docker + Python 3.11 + herramientas de calidad
- **Criterio**: `make setup` instala todo sin errores
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Comandos**:
  ```bash
  python -m venv venv && source venv/bin/activate
  pip install ruff black mypy pytest pre-commit
  pre-commit install
  ```

### [INFRA-002] Estructura de repositorio completa
- **Resultado**: Estructura de carpetas con READMEs en cada servicio
- **Criterio**: `tree -L 3` muestra estructura esperada
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Dependencia**: INFRA-001

### [INFRA-003] Configuraci√≥n MCP inicial
- **Resultado**: MCP servers b√°sicos funcionando
- **Criterio**: `mcp list` muestra servers disponibles
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Dependencia**: INFRA-002

## üß† √âpica: Ontolog√≠a + Knowledge Graph

### [KG-001] Dise√±ar ontolog√≠a OWL 2 DL
- **Resultado**: Ontolog√≠a con clases: Child, Conversation, Topic, Utterance, Emotion, SkillLevel
- **Criterio**: Validaci√≥n RDF Schema sin errores
- **Due√±o**: Ontology Curator
- **Estimaci√≥n**: M
- **Comandos**:
  ```bash
  robot validate -i ontology.ttl -o report.txt
  ```

### [KG-002] Crear formas SHACL
- **Resultado**: 5-8 reglas de validaci√≥n SHACL
- **Criterio**: `shacl validate` aprueba ejemplos correctos, rechaza incorrectos
- **Due√±o**: Ontology Curator
- **Estimaci√≥n**: M
- **Dependencia**: KG-001

### [KG-003] Configurar Fuseki Docker
- **Resultado**: Contenedor Fuseki con datos de prueba
- **Criterio**: SPARQL endpoint responde en http://localhost:3030
- **Due√±o**: KG IO Agent
- **Estimaci√≥n**: S
- **Comandos**:
  ```bash
  docker run -d -p 3030:3030 stain/jena-fuseki
  ```

### [KG-004] ‚úÖ Implementar API /kg
- **Resultado**: ‚úÖ Endpoints: insert, update, retrieve, reason/check
- **Criterio**: ‚úÖ Tests contractuales con schemathesis pasan
- **Due√±o**: KG IO Agent
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - API SPARQL completa con validaci√≥n SHACL
- **Dependencia**: KG-003

### [KG-005] Integrar HermiT reasoner
- **Resultado**: Batch job que verifica consistencia OWL
- **Criterio**: Detecta inconsistencias en datos de prueba
- **Due√±o**: Ontology Curator
- **Estimaci√≥n**: M
- **Dependencia**: KG-004

### [KG-006] ‚è≥ Crear ontolog√≠a de memoria (memoria.ttl) **NEW**
- **Resultado**: SHACL shapes para MemoryEntry, ExtractedEntity, Interest
- **Criterio**: Validaci√≥n de embeddings y entidades extra√≠das
- **Due√±o**: Ontology Curator
- **Estimaci√≥n**: S (4 horas)
- **Estado**: PENDIENTE - Requisito para MEM-003
- **Archivos nuevos**:
  - `ontology/memoria.ttl` - SHACL shapes para memoria
  - `ontology/entities.ttl` - Definiciones de tipos de entidades
- **Contenido**: Ver PRD.md Anexo "SHACL Shapes - memoria.ttl"
- **Clases**:
  - `emo:MemoryEntry` - Entrada de memoria con embedding
  - `emo:ExtractedEntity` - Entidad extra√≠da (Activity, Place, Person, Object)
  - `emo:Interest` - Inter√©s inferido del ni√±o
- **Propiedades**:
  - `emo:hasMemoryEntry` - Link child ‚Üí memory entries
  - `emo:embedding` - Vector embedding (JSON string)
  - `emo:relevanceScore` - Score 0.0-1.0
  - `emo:entityType`, `emo:value`, `emo:confidence`

### [KG-007] ‚è≥ Crear prompt de extracci√≥n de entidades **NEW**
- **Resultado**: Prompt template para extractor LLM
- **Criterio**: JSON output con entities, emotions, relationships, topics
- **Due√±o**: Conversation Coach
- **Estimaci√≥n**: XS (2 horas)
- **Estado**: PENDIENTE - Requisito para MEM-003
- **Archivos nuevos**:
  - `.claude/prompts/extract_entities.md` - Prompt template
- **Contenido**: Ver PRD.md Anexo "Prompt de extracci√≥n de entidades"
- **Output esperado**:
  ```json
  {
    "entities": [{"type": "Activity", "value": "f√∫tbol", "confidence": 0.9}],
    "emotions": [{"type": "positive", "intensity": 0.8}],
    "relationships": [{"subject": "child", "predicate": "likes", "object": "f√∫tbol"}],
    "topics": ["hobbies", "sports"]
  }
  ```

## üí¨ √âpica: API /conv (conversaci√≥n)

### [CONV-000] ‚è≥ Refactor LLM service para API vLLM **CRITICAL**
- **Resultado**: LLM service usa HTTP client para atacar vLLM API (port 8001)
- **Criterio**: Eliminar import directo de vllm, usar httpx/requests
- **Due√±o**: Conversation Coach
- **Estimaci√≥n**: S (4 horas)
- **Estado**: PENDIENTE - Cr√≠tico para arquitectura correcta
- **Archivos a modificar**:
  - `services/api/services/llm_service.py` - Reemplazar AsyncLLMEngine con httpx client
  - `services/api/core/config.py` - Agregar vllm_api_url = "http://localhost:8001"
  - `services/api/requirements.txt` - Agregar httpx, remover vllm si est√°
- **Comandos**:
  ```bash
  # Servidor vLLM separado (no en API container)
  docker run --gpus all -p 8001:8000 vllm/vllm-openai:latest \
    --model Qwen/Qwen2-7B-Instruct
  ```
- **Implementaci√≥n**:
  ```python
  # services/api/services/llm_service.py
  import httpx
  
  class LLMService:
      def __init__(self):
          self.api_url = settings.vllm_api_url
          self.client = httpx.AsyncClient(timeout=30.0)
      
      async def generate_response(self, prompt: str, **kwargs):
          response = await self.client.post(
              f"{self.api_url}/v1/completions",
              json={
                  "model": settings.llm_model,
                  "prompt": prompt,
                  "max_tokens": kwargs.get("max_tokens", 150),
                  "temperature": kwargs.get("temperature", 0.7)
              }
          )
          return response.json()["choices"][0]["text"]
  ```

### [CONV-001] Modelo de datos MongoDB
- **Resultado**: Collections: children, conversations, messages, topics, profiles
- **Criterio**: Migraci√≥n inicial con datos de prueba
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Comandos**:
  ```bash
  mongo emorobcare --eval "db.children.insertOne({name: 'test_child'})"
  ```

### [CONV-002] Endpoints b√°sicos /conv
- **Resultado**: POST /conv/start, POST /conv/next, GET /conv/{id}
- **Criterio**: Respuestas en formato JSON con emotion markup
- **Due√±o**: Conversation Coach
- **Estimaci√≥n**: M
- **Dependencia**: CONV-001

### [CONV-003] ‚úÖ LLM local con vLLM (API Integration)
- **Resultado**: Servicio vLLM con Qwen2-7B-Instruct accesible v√≠a API
- **Criterio**: ‚úÖ API OpenAI-compatible en port 8001, generaci√≥n < 2s
- **Due√±o**: Conversation Coach
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - vLLM API integrado con monitoreo de rendimiento
- **IMPORTANTE**: LLM service debe atacar vLLM v√≠a HTTP API, no importar directamente
- **Comandos**:
  ```bash
  # Servidor vLLM (separado)
  python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct --port 8001
  
  # Cliente en services/api/services/llm_service.py
  import httpx
  response = await httpx.post("http://localhost:8001/v1/completions", json={...})
  ```

### [CONV-004] ‚úÖ Sistema de niveles conversacionales
- **Resultado**: ‚úÖ L√≥gica que adapta respuestas seg√∫n nivel 1-5 implementado
- **Criterio**: ‚úÖ Tests unitarios para cada nivel completados
- **Due√±o**: Conversation Coach
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - Niveles 1-5 con complejidad apropiada
- **Dependencia**: CONV-003

### [CONV-005] Markup emocional
- **Resultado**: Sistema que aplica **feliz**, __susurro__, neutral
- **Criterio**: Validaci√≥n de markup en respuestas
- **Due√±o**: Safety & Tone Guardian
- **Estimaci√≥n**: S
- **Estado**: COMPLETADO - Markup integrado en LLM y validado
- **Dependencia**: CONV-004

### [CONV-006] ‚úÖ Guardrails de seguridad
- **Resultado**: ‚úÖ Validaci√≥n de contenido inapropiado completada
- **Criterio**: ‚úÖ Tests de seguridad cubren edge cases
- **Due√±o**: Safety & Tone Guardian
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - Sistema completo de seguridad infantil
- **Dependencia**: CONV-005

## üé§ √âpica: API /asr (speech-to-text)

### [ASR-001] ‚úÖ Setup faster-whisper
- **Resultado**: ‚úÖ Servicio ASR b√°sico con Whisper completado
- **Criterio**: ‚úÖ Transcripci√≥n b√°sica funcional
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Estado**: COMPLETADO - faster-whisper integrado
- **Comandos**:
  ```bash
  pip install faster-whisper
  ```

### [ASR-002] ‚úÖ Implementar 3 niveles de precisi√≥n
- **Resultado**: ‚úÖ fast, balanced, accurate con diferentes modelos
- **Criterio**: ‚úÖ Benchmarks de latencia/precisi√≥n alcanzados
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - Sistema de 3 niveles funcionando
- **Dependencia**: ASR-001

### [ASR-003] ‚úÖ API /asr completa
- **Resultado**: ‚úÖ POST /asr/transcribe con par√°metro tier
- **Criterio**: ‚úÖ Tests con archivos de audio variados completados
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Estado**: COMPLETADO - API completa con documentaci√≥n
- **Dependencia**: ASR-002

### [ASR-004] ‚úÖ Optimizaci√≥n GPU
- **Resultado**: ‚úÖ Aceleraci√≥n con CUDA si disponible
- **Criterio**: ‚úÖ Benchmark muestra speedup > 2x
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Estado**: COMPLETADO - Detecci√≥n autom√°tica de GPU
- **Dependencia**: ASR-003

## üß† √âpica: Memoria + Retrieval (RAG)

### [MEM-001] ‚úÖ Integrar Qdrant
- **Resultado**: ‚úÖ Servicio Qdrant Docker corriendo
- **Criterio**: ‚úÖ Collections creadas para conversaciones
- **Due√±o**: Memory/Retrieval Agent
- **Estimaci√≥n**: S
- **Estado**: COMPLETADO - Qdrant integrado con configuraci√≥n √≥ptima
- **Comandos**:
  ```bash
  docker run -p 6333:6333 qdrant/qdrant
  ```

### [MEM-002] ‚úÖ Embedding de conversaciones
- **Resultado**: ‚úÖ Vectorizaci√≥n de utterances con embeddings
- **Criterio**: ‚úÖ B√∫squeda sem√°ntica por ni√±o+tema funcionando
- **Due√±o**: Memory/Retrieval Agent
- **Estimaci√≥n**: M
- **Estado**: COMPLETADO - Sistema multiling√ºe con optimizaci√≥n
- **Dependencia**: MEM-001

### [MEM-003] ‚è≥ Pipeline de extracci√≥n as√≠ncrona **HIGH PRIORITY**
- **Resultado**: BackgroundTasks ‚Üí vLLM extractor API ‚Üí entidades ‚Üí validaci√≥n SHACL ‚Üí triples Fuseki
- **Criterio**: Extracci√≥n KG sin bloquear conversaci√≥n, < 30s por conversaci√≥n
- **Due√±o**: KG IO Agent
- **Estimaci√≥n**: M (16 horas)
- **Dependencia**: KG-004 ‚úÖ, MEM-002 ‚úÖ
- **Estado**: PENDIENTE - Ver TODOS/01-MEMORY_RETRIEVAL_EPIC.md para detalles
- **Archivos nuevos**:
  - `services/api/services/extraction_service.py` - Servicio de extracci√≥n
  - `services/api/routers/background_tasks.py` - Manejo de tareas async
  - `services/api/models/extraction_models.py` - Modelos de extracci√≥n
  - `ontology/memoria.ttl` - SHACL shapes para memoria y entidades
  - `.claude/prompts/extract_entities.md` - Prompt de extracci√≥n
- **Subtareas**:
  - [ ] Implementar ExtractionService con llamadas a vLLM API
  - [ ] Crear SHACL shapes para entidades extra√≠das (memoria.ttl)
  - [ ] Pipeline: conversaci√≥n ‚Üí extractor ‚Üí RDF ‚Üí validaci√≥n ‚Üí Fuseki
  - [ ] Manejo de errores y logging completo
  - [ ] Tests unitarios e integraci√≥n

### [MEM-004] ‚è≥ RAG en /conv **HIGH PRIORITY**
- **Resultado**: Context retrieval desde Qdrant + KG previo a generar respuesta
- **Criterio**: Context retrieval < 500ms, A/B testing muestra mejora en coherencia
- **Due√±o**: Memory/Retrieval Agent
- **Estimaci√≥n**: M (12 horas)
- **Dependencia**: MEM-003
- **Estado**: PENDIENTE - Integrar b√∫squeda sem√°ntica en pipeline conversacional
- **Archivos a modificar**:
  - `services/api/services/memory_service.py` - Extend context retrieval
  - `services/api/routers/conversation.py` - Integrar RAG en /conv/next
  - `services/api/models/conversation_models.py` - Modelos con contexto
- **Subtareas**:
  - [ ] Implementar get_conversation_context() con Qdrant + KG fusion
  - [ ] Ranking de contexto por relevancia (score threshold)
  - [ ] Formateo de contexto para prompt LLM
  - [ ] Integraci√≥n en /conv/next con performance < 500ms
  - [ ] A/B testing y m√©tricas de coherencia

## üñ•Ô∏è √âpica: Frontend Vue 3

### [UI-001] Proyecto Vue 3 base
- **Resultado**: Estructura Vite + Vue 3 funcionando
- **Criterio**: npm run dev funciona en port 81
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Comandos**:
  ```bash
  cd services/frontend && npm create vue@latest .
  ```

### [UI-002] Componente conversaci√≥n
- **Resultado**: Chat interface con emotion markup
- **Criterio**: Renderizado correcto de **feliz**, __susurro__
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Dependencia**: UI-001

### [UI-003] Grabaci√≥n de audio
- **Resultado**: Componente que graba y env√≠a a /asr
- **Criterio**: WebRTC API integrada
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Dependencia**: UI-002, ASR-003

### [UI-004] Navegaci√≥n ni√±os/temas
- **Resultado**: Browse children, topics, conversations
- **Criterio**: CRUD b√°sico desde UI
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Dependencia**: UI-003

### [UI-005] Biling√ºe ES/EN
- **Resultado**: Switch idioma en UI
- **Criterio**: Textos localizados correctamente
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Dependencia**: UI-004

## üß™ √âpica: Testing & Calidad

### [TEST-001] Unit tests API
- **Resultado**: Cobertura > 80% en servicios principales
- **Criterio**: pytest --cov report
- **Due√±o**: Evaluator/Tester Agent
- **Estimaci√≥n**: M
- **Comandos**:
  ```bash
  pytest --cov=services/api tests/
  ```

### [TEST-002] Contract tests OpenAPI
- **Resultado**: Validaci√≥n autom√°tica de contratos API
- **Criterio**: schemathesis run sin fallos
- **Due√±o**: Evaluator/Tester Agent
- **Estimaci√≥n**: M
- **Dependencia**: TEST-001

### [TEST-003] E2E tests
- **Resultado**: Flujo completo conversaci√≥n + KG + ASR
- **Criterio**: Tests reproducibles con Docker compose
- **Due√±o**: Evaluator/Tester Agent
- **Estimaci√≥n**: L
- **Dependencia**: TEST-002

### [TEST-004] Linting + type checking
- **Resultado**: ruff, black, mypy configurados
- **Criterio**: pre-commit hooks funcionando
- **Due√±o**: Evaluator/Tester Agent
- **Estimaci√≥n**: S
- **Comandos**:
  ```bash
  ruff check . && black --check . && mypy services/
  ```

### [TEST-005] Performance tests
- **Resultado**: Benchmarks de latencia y throughput
- **Criterio**: < 2s respuesta conversaci√≥n local
- **Due√±o**: Evaluator/Tester Agent
- **Estimaci√≥n**: M
- **Dependencia**: TEST-004

## üê≥ √âpica: Docker & Deploy

### [DOCKER-001] Dockerfiles multi-stage
- **Resultado**: Im√°genes < 500MB por servicio (excepto vLLM que es externo)
- **Criterio**: docker images muestra tama√±os
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Nota**: vLLM corre como servicio separado, NO dentro del API container
- **Comandos**:
  ```bash
  # API service (sin vLLM)
  docker build -t emorobcare-api -f services/api/Dockerfile .
  
  # vLLM service (separado, GPU)
  docker run --gpus all -p 8001:8000 vllm/vllm-openai:latest \
    --model Qwen/Qwen2-7B-Instruct
  ```
- **docker-compose.yml debe incluir**:
  ```yaml
  services:
    vllm:
      image: vllm/vllm-openai:latest
      ports:
        - "8001:8000"
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
      command: --model Qwen/Qwen2-7B-Instruct
    
    api:
      build: ./services/api
      ports:
        - "8000:8000"
      environment:
        - VLLM_API_URL=http://vllm:8000
      depends_on:
        - vllm
        - mongodb
        - qdrant
        - fuseki
  ```

### [DOCKER-002] Build y run scripts
- **Resultado**: Scripts automatizados para despliegue
- **Criterio**: make deploy levanta todos servicios
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: S
- **Dependencia**: DOCKER-001

### [DOCKER-003] Optimizaci√≥n offline
- **Resultado**: Modelos descargados en build time
- **Criterio**: Funciona completamente sin internet
- **Due√±o**: Planner-Orchestrator
- **Estimaci√≥n**: M
- **Dependencia**: DOCKER-002

## üìã Entregables por sprint

### Sprint 1 (2 semanas) - Base funcional
- [x] INFRA-001, INFRA-002
- [x] CONV-001, CONV-002
- [x] KG-001, KG-002
- [x] TEST-004

### Sprint 2 (2 semanas) - Integraci√≥n inicial
- [x] CONV-003, CONV-004
- [x] ASR-001, ASR-002, ASR-003, ASR-004
- [x] MEM-001, MEM-002
- [x] KG-003, KG-004
- [x] TEST-001

### Sprint 3 (2 semanas) - Memory integration + testing ‚è≥ **CURRENT**
- [ ] **CONV-000** - Refactor LLM service para API vLLM (CR√çTICO)
- [ ] **KG-006** - Crear ontolog√≠a de memoria (memoria.ttl)
- [ ] **KG-007** - Crear prompt de extracci√≥n de entidades
- [ ] **MEM-003** - Pipeline de extracci√≥n as√≠ncrona
- [ ] **MEM-004** - RAG en /conv con context retrieval
- [ ] CONV-005, CONV-006 - Markup emocional y guardrails
- [ ] UI-001, UI-002, UI-003 - Frontend b√°sico
- [ ] TEST-002, TEST-003 - Contract tests + E2E

### Sprint 4 (1 semana) - Optimizaci√≥n + deploy
- [ ] DOCKER-001, DOCKER-002, DOCKER-003
- [ ] KG-005 - HermiT reasoner
- [ ] UI-004, UI-005 - Navegaci√≥n y biling√ºe
- [ ] TEST-005 - Performance tests

---

## üöÄ Comandos √∫tiles

```bash
# Setup completo
make setup

# Tests completos
make test-all

# Deploy local
make deploy-local

# Linting
make lint

# API docs
make docs
```